[main]
debug = false
# Load non-minified versions of js libraries, if not false
debug_js = false
# Trace ExtJS events
trace_extjs_events = false
# Traceback order. One of
# forward -- top of stack last
# reverse -- top of stack first
traceback_order = reverse
# Comma-separated list of emails to send error reports
admin_emails =
# Local time zone for this installation. Choices can be found here:
# http://www.postgresql.org/docs/8.1/static/datetime-keywords.html#DATETIME-TIMEZONE-SET-TABLE
# although not all variations may be possible on all operating systems.
# If running in a Windows environment this must be set to the same as your
# system time zone.
timezone = Europe/Moscow
#
# Date Format:
# Allowed formats: http://docs.djangoproject.com/en/dev/ref/templates/builtins/#ttag-now
#
date_format = d.m.Y
#
# Time format.
# Allowed formats: http://docs.djangoproject.com/en/dev/ref/templates/builtins/#ttag-now
time_format = H:i:s
# Month/Day format
# Allowed formats: http://docs.djangoproject.com/en/dev/ref/templates/builtins/#ttag-now
month_day_format = F j
# Month/Year format
# Allowed formats: http://docs.djangoproject.com/en/dev/ref/templates/builtins/#ttag-now
year_month_format = F Y
#
# DateTime format:
# Allowed formats: http://docs.djangoproject.com/en/dev/ref/templates/builtins/#ttag-now
#
datetime_format = d.m.Y H:i:s
# Language code for this installation. All choices can be found here:
# http://www.w3.org/TR/REC-html40/struct/dirlang.html#langcodes
# http://blogs.law.harvard.edu/tech/stories/storyReader$15
language_code = en-us
# Make this unique, and don't share it with anybody.
secret_key = j82icp#5zBUZ!4hx^#0s4)dy8sru@1ynqblq2!1lv1lu=7&(58
# From: for server-generated emails
server_email = root@localhost
# Socket factory polling method.
# Possible selections are:
# * optimal - auto-detect best supported method
# * select - use select()
# * poll   - use poll()
# * kevent - use kevent/kqueue
polling_method = select
# Comma-separated list of additionally installed applications
installed_apps =
# Maximum crashinfo size (in bytes)
crashinfo_limit = 1000000
# Header contining an original protocol, when changed by reverse proxy
# usually:
# x_forwarded_proto = X-Forwarded-Proxy
x_forwarded_proto =
# Log REST API calls and input parameters
log_api_calls = false
# Log SQL statements
log_sql_statements = false
# If set, return "Request too large" if json request
# is larger than limit (0 - unlimited)
json_row_limit = 0

[solutions]
# Enable noc.default solution
noc.default = true

[database]
# postgresql_psycopg2
engine   = postgresql_psycopg2
# Database name
name     = noc
# Database user
user     = noc
# User password
password = noc
# Set to empty string for localhost
host     =
# Set to empty string for default
port     =

[nosql_database]
# MongoDB database name
name = noc
# MongoDB database user
user = noc
# MongoDB database password
password = noc
#
host =
#
port =
# Mongo replica set name for HA configuration
replica_set =

#
# NOC User Authentication
#
[authentication]
#
# Authentication method. Must be one of:
# local - use NOC's database to authenticate users
# ldap - use LDAP server to authenticate users (python-ldap library required)
# ad - use MS Active Directory (python-ldap library required)
# http - use HTTP's REMOTE_USER for authentication
# pyrule - use custom pyRule for authentication

method = local
#
# pyRule, returning login form class.
# Empty to use default auth_form_user_password
#
form_pyrule = 

# LDAP server URI (Applicable only for "ldap" method)
# If your LDAP server supports partitions (i.e Apache DS),
# specify suffix in the URI: ldap://ldap.exaple.com/suffix
ldap_server = ldap://ldap.example.com/
# LDAP bind method:
#     simple - authenticate user against userPassword
ldap_bind_method = simple
# technical DN to lookup user and group information
# leave empty for anonymous bind
ldap_bind_dn =
# Password for technical DN
ldap_bind_password = 
# Users search base
ldap_users_base = ou=Users,ou=nocproject,ou=org
# Filter to search users by username
ldap_users_filter = (&(objectClass=inetOrgPerson)(uid={{user}}))
# Groups search base
ldap_groups_base = ou=Groups,ou=nocproject,ou=org
# Additionaly check user in group, before permitting to log in
# Set user.is_active flag to false when user is not in a group
ldap_required_group = 
# Filter to search user in required group
ldap_required_filter = (|(uniqueMember={{dn}})(member={{user}}))
# Grant superuser permissions if user in group
ldap_superuser_group =
# Filter to search user in superusers group
ldap_superuser_filter = (|(uniqueMember={{dn}})(member={{user}}))
# Use TLS
ldap_start_tls = false

# AD server URI (Applicable only for "ad" method)
ad_server = ldap://ad.example.com/
# AD bind method:
#     simple - authenticate user against userPassword
ad_bind_method = simple
# technical DN to lookup user and group information
# leave empty for anonymous bind
ad_bind_dn =
# Password for technical DN
ad_bind_password = 
# Users search base
ad_users_base = ou=nocproject,ou=org
# Filter to search users by username
ad_users_filter = (&(objectClass=organizationalPerson)(sAMAccountName={{user}}))
# Groups search base
ad_groups_base = ou=Groups,ou=nocproject,ou=org
# Additionaly check user in group, before permitting to log in
# Set user.is_active flag to false when user is not in a group
ad_required_group = 
# Filter to search user in required group
ad_required_filter = noc_users
# Grant superuser permissions if user in group
ad_superuser_group = noc_superusers
# Filter to search user in superusers group
ad_superuser_filter =

# Authentication pyrule (for pyrule method)
pyrule_authentication =

# If group name given, permit logins only from specified group
restrict_to_group =

# If group name given, only one active session per group member is allowed
single_session_group =

# If group name given, only one active session per all group member is allowed
mutual_exclusive_group =
# If set, auto-logout after idle_timeout secods of inactivity
idle_timeout = 0

#
# Site customization
#
[customization]
# Installation name. Displayed in top left corner of web interface
installation_name = Unconfigured Installation
# Logo url. Absolute or relative path
logo_url = /static/img/logo_black.svg
# Logo width
logo_width = 24
# Logo height
logo_height = 24
# favicon url
favicon_url = /static/img/logo_24x24_deep_azure.png
# Default theme
default_theme = gray
# Top panel colors
branding_background_color = #c0c0c0
branding_color = #000

#
# UI themes
#
[themes]
neptune.name = Neptune
neptune.enabled = true

blue.name = Blue
blue.enabled = true

gray.name = Gray
gray.enabled = true

access.name = Access
access.enabled = true

crisp.name = Crisp
crisp.enabled = true

#
# cm module setup
#
[cm]
# Path to the root of the repos
repo     = /var/repo
# VCS Type: hg, CVS
vcs_type = hg
# Path to VCS CLI utility
vcs_path = /usr/local/bin/hg
# Limit amount of concurrently grabbed configs
# 0 - no limit
concurrency = 0
# Next check on activator overload
timeout_overload = 150
# Next check on down host
timeout_down     = 60
# Next check after config fetching error
timeout_error    = 300
# Random variation to timeout.
# If timeout is T and variation is N means
# the final timeout will be random value in range of [T-T/N,T+T/N]
timeout_variation = 10

[gridvcs]
# Mirror sa.managedobject.config to designated directory
# Do not mirror when empty
mirror.sa.managedobject.config =

#
# Peer module setup
#
[peer]
# RPSL pref semantics
#   off -> pref == localpref
#   on  -> pref == 65535 - localpref
rpsl_inverse_pref_style = off
#
# Enable prefix list optimization
#
prefix_list_optimization = on
#
# Prefix list optimization threshold.
# Do not optimize prefix lists shorter than threshold
#
prefix_list_optimization_threshold = 0
#
# Maximal prefix length for generated prefix lists
#
max_prefix_length = 32
#
# Update whois cache from RIPE database
#
use_ripe = on
#
# Update whois cache from ARIN database
#
use_arin = on
#
# Update whois cache from RADb database
#
use_radb = on

#
# DNS Module setup
#
[dns]
# Start to issue expiration warnings from warn_before_expired_days before deadline
warn_before_expired_days = 30
# Zone update delay
delay = 5
# Update zone if delayed more than cutoff
cutoff = 3600

#
# Trouble Ticketing System integration
#
[tt]
# Link to TT. %(tt)s expanded into ticket number
url = http://example.com/ticket=%%(tt)s

#
# Other paths
#
[path]
# Directory to store database and repo backups
backup_dir = /var/backup
# Paths to utilities
ssh        = /usr/bin/ssh
rsync      = /usr/bin/rsync
pg_dump    = /usr/local/bin/pg_dump
tar        = /usr/bin/tar
gzip       = /usr/bin/gzip
smidump    = /usr/bin/smidump
smilint    = /usr/bin/smilint
dig        = /usr/bin/dig
gpg        = /usr/bin/gpg
mongodump  = /usr/bin/mongodump
#
# Backup parameters
#
[backup]
keep_days          = 14
keep_weeks         = 12
keep_day_of_week   = 6
keep_months        = 12
keep_day_of_month  = 1
#
# Developers settings
#
[develop]
# Enable <install collection> button
install_collection = false

#
# PGP Settings
#
[pgp]
use_key   = user@example.com
keyserver = keys.gnupg.net
#
# FM Settings
#
[fm]
# Time to store active event in the noc.event.active collection
# before moving to archive
active_window = 86400
# Archive settings:
# Keep events without alarms in archive for X days
# 0 - do not archive
# -1 - keep forever
keep_events_wo_alarm = 0
# Archive settings:
# Keep events with alarms in archive for X days
# 0 - do not archive
# -1 - keep forever
keep_events_with_alarm = -1

#
# GIS Settings
#
[gis]
# Ellipsoids for distance calculations
# Possible values:
#     WGS-84
#     Krass
#     ПЗ-90
ellipsoid = ПЗ-90
# Enable OpenStreetMap base layer
enable_osm = true
# Enable Google Sattelite base layer
enable_google_sat = false
# Eanble Google Roadmap base layer
enable_google_roadmap = false

#
# Proxy settings
#
[proxy]
http_proxy =
https_proxy =
ftp_proxy =
#
# Cache settings
#
[cache]
sa_managedobjectselector_object_ids = 60
vc_vcinterfacescount = 3600
vc_vcprefixes = 3600

[version_inventory]
# Enable version discovery process
enabled = true
# Save result to database
save = true
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[config_discovery]
# Enable config discovery process
enabled = true
# Save result to database
save = true
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[interface_discovery]
enabled = true
save = true
success_retry = 86400
failed_retry = 900
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20
# IInterfaceClassification-interfaced pyRule
# returning interface class name
# Used by noc.solutions.noc.default.discovery.interface.get_interface_profile
classification_pyrule =
# Description generator
get_interface_profile = noc.solutions.noc.default.discovery.interface.get_interface_profile


[prefix_discovery]
enabled = true
save = true
# Change prefix state when found
# Format: from state -> to state; ...; from state -> to state
# Example:
# change_state = PLANNED -> ALLOCATED; RESERVED -> ALLOCATED
change_state =
# IGetDiscoveryCustom pyRule name to set up prefix custom fields
custom_pyrule =

[ip_discovery]
# Enable ip discovery process
enabled = true
# True - save results to IPAM database
# False - only report new addresses
save = true
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20
# Change address state when found
# Format: from state -> to state; ...; from state -> to state
# Example:
# change_state = PLANNED -> ALLOCATED; RESERVED -> ALLOCATED
change_state =
#
# Django template to generate discovered Address'es FQDN.
# USed by noc.solutions.noc.default.discovery.ip.get_fqdn solution
# Available context variables are:
#   object - Managed object instance
#   vrf - VRF instance
#   name - Managed object's name
#   host - host part of managed object's name
#   domain - domain part of managed object's name
#   interface - interface, on which address have been discovered
#   afi - address family, "4" or "6"
#   ip - ip address
#   IP - list of octets for IPv4 of 16-bit blocks for IPv6
#   rIP - IP in reversed order
#
fqdn_template = ip-{{ IP|join:"-" }}.{%% if domain %%}{{ domain }}{%% else %%}example.com{%% endif %%}
# Description generator
get_description = noc.solutions.noc.default.discovery.ip.get_description
# FQDN generator
get_fqdn = noc.solutions.noc.default.discovery.ip.get_fqdn

[vlan_discovery]
enabled = true
save = true
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20
# change_state = PLANNED -> ALLOCATED; RESERVED -> ALLOCATED
change_state =

[mac_discovery]
enabled = true
save = true
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[id_discovery]
enabled = true
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[asset_discovery]
enabled = true
save = true
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20
# Object name generator
get_name = noc.solutions.noc.default.discovery.asset.get_name

[lldp_discovery]
enabled = false
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[cdp_discovery]
enabled = false
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[stp_discovery]
enabled = false
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[rep_discovery]
enabled = false
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[bfd_discovery]
enabled = false
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[udld_discovery]
enabled = false
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[oam_discovery]
enabled = false
# Inject new schedules every N seconds
initial_submit_interval = 900
# Upper limit of new schedule submitted at once
initial_submit_concurrency = 20

[audit]
# Store commands audit log
command_ttl = 1m
# Store login/logout audit log
login_ttl = 1m
# Store reboot/started/halted log
reboot_ttl = 0
# Store config changed log
config_changed_ttl = 1y

[pm_storage]
# PM storage type
# Available types are:
#    whisper
#    ceres
type = whisper

[storage_whisper]
# Whisper storage settings
# Storage root directory
data_dir = local/whisper
# Create sparse file
sparse_create = false
#
autoflush = false
#
lock_writes = false

[storage_ceres]
# Ceres storage settings
# Storage root directory
data_dir = local/ceres
# Ceres nodes can have many slices and caching the right ones can improve
# performance dramatically. Note that there are many trade-offs to tinkering
# with this, and unless you are a ceres developer you *really* should not
# mess with this. Valid values are:
#    latest - only the most recent slice is cached
#       all - all slices are cached
#      none - slice caching is disabled
slice_caching = latest

# If a Ceres node accumulates too many slices, performance can suffer.
# This can be caused by intermittently reported data. To mitigate
# slice fragmentation there is a tolerance for how much space can be
# wasted within a slice file to avoid creating a new one. That tolerance
# level is determined by MAX_SLICE_GAP, which is the number of consecutive
# null datapoints allowed in a slice file.
# If you set this very low, you will waste less of the *tiny* bit disk space
# that this feature wastes, and you will be prone to performance problems
# caused by slice fragmentation, which can be pretty severe.
# If you set this really high, you will waste a bit more disk space (each
# null datapoint wastes 8 bytes, but keep in mind your filesystem's block
# size). If you suffer slice fragmentation issues, you should increase this or
# run the ceres-maintenance defrag plugin more often. However you should not
# set it to be huge because then if a large but allowed gap occurs it has to
# get filled in, which means instead of a simple 8-byte write to a new file we
# could end up doing an (8 * MAX_SLICE_GAP)-byte write to the latest slice.
max_slice_gap = 80